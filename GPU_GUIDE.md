# H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng GPU

## ‚úÖ GPU ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t th√†nh c√¥ng!

**Th√¥ng tin GPU c·ªßa b·∫°n:**
- GPU: NVIDIA GeForce RTX 3050 Laptop GPU
- VRAM: 4GB
- CUDA Version: 12.3 (Driver)
- PyTorch CUDA: 11.8 (Compatible)

---

## üöÄ Ch·∫°y Inference v·ªõi GPU

### C√°ch 1: T·ª± ƒë·ªông (M·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng GPU)

Pipeline s·∫Ω **t·ª± ƒë·ªông** s·ª≠ d·ª•ng GPU n·∫øu c√≥:

```powershell
# GPU s·∫Ω ƒë∆∞·ª£c d√πng t·ª± ƒë·ªông (PowerShell - m·ªôt d√≤ng)
python run.py infer --dataset_dir ./train --output predictions_gpu.json --detector yolov8m.pt --threshold 0.45 --conf 0.2 --skip 2
```

### C√°ch 2: Ch·ªâ ƒë·ªãnh r√µ r√†ng

```powershell
# Ch·ªâ ƒë·ªãnh d√πng GPU
python run.py infer --dataset_dir ./train --output predictions_gpu.json --detector yolov8m.pt --device cuda
```

### C√°ch 3: B·∫Øt bu·ªôc d√πng CPU (n·∫øu c·∫ßn)

```powershell
# B·∫Øt bu·ªôc d√πng CPU
python run.py infer --dataset_dir ./train --output predictions_cpu.json --detector yolov8m.pt --device cpu
```

---

## ‚ö° T·ªëc ƒë·ªô C·∫£i thi·ªán v·ªõi GPU

**So s√°nh CPU vs GPU:**

| C·∫•u h√¨nh | CPU (Core i5/i7) | GPU (RTX 3050) | TƒÉng t·ªëc |
|----------|------------------|----------------|----------|
| YOLOv8n + CLIP | ~5 fps | ~15 fps | **3x** |
| YOLOv8m + CLIP | ~2 fps | ~10 fps | **5x** |
| YOLOv8x + CLIP | ~0.5 fps | ~5 fps | **10x** |
| YOLOv8x + Both | ~0.3 fps | ~3 fps | **10x** |

---

## üéØ C·∫•u h√¨nh ƒê·ªÅ xu·∫•t cho GPU 4GB

### C·∫•u h√¨nh T·ªëi ∆∞u (C√¢n b·∫±ng t·ªëc ƒë·ªô & ch·∫•t l∆∞·ª£ng)

```powershell
python run.py infer --dataset_dir ./train --output predictions_optimal.json --model clip --detector yolov8l.pt --threshold 0.45 --conf 0.2 --skip 1
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ T·∫≠n d·ª•ng GPU hi·ªáu qu·∫£
- ‚úÖ Kh√¥ng b·ªã Out of Memory
- ‚úÖ Ch·∫•t l∆∞·ª£ng t·ªët (~0.4-0.6 STIoU)
- ‚úÖ T·ªëc ƒë·ªô: ~8-10 fps

### C·∫•u h√¨nh Ch·∫•t l∆∞·ª£ng Cao (Ch·∫≠m h∆°n nh∆∞ng ch√≠nh x√°c)

```powershell
python run.py infer --dataset_dir ./train --output predictions_high_quality.json --model both --detector yolov8x.pt --threshold 0.4 --conf 0.15 --skip 1
```

**L∆∞u √Ω:**
- ‚ö†Ô∏è C√≥ th·ªÉ h·∫øt VRAM n·∫øu video resolution cao
- ‚ö†Ô∏è T·ªëc ƒë·ªô: ~3-5 fps
- ‚úÖ Ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t (~0.5-0.7 STIoU)

### C·∫•u h√¨nh Nhanh (ƒê·ªÉ test)

```powershell
python run.py infer --dataset_dir ./train --output predictions_fast.json --model clip --detector yolov8m.pt --threshold 0.5 --conf 0.25 --skip 2
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ R·∫•t nhanh: ~10-15 fps
- ‚úÖ Ti·∫øt ki·ªám VRAM
- ‚ö†Ô∏è Ch·∫•t l∆∞·ª£ng trung b√¨nh (~0.3-0.5 STIoU)

---

## üí° Tips T·ªëi ∆∞u GPU 4GB

### 1. Gi·∫£m Batch Size (n·∫øu c·∫ßn)
Pipeline m·∫∑c ƒë·ªãnh x·ª≠ l√Ω t·ª´ng frame, ƒë√£ t·ªëi ∆∞u cho 4GB VRAM.

### 2. Tr√°nh Out of Memory

**N·∫øu g·∫∑p l·ªói CUDA Out of Memory:**

```powershell
# Gi·∫£m xu·ªëng model nh·ªè h∆°n
python run.py infer --dataset_dir ./train --output predictions.json --detector yolov8m.pt
```

Ho·∫∑c:

```powershell
# D√πng ch·ªâ CLIP thay v√¨ 'both'
python run.py infer --dataset_dir ./train --output predictions.json --model clip
```

### 3. Theo d√µi VRAM Usage

M·ªü terminal m·ªõi v√† ch·∫°y:
```bash
# Xem GPU usage real-time
nvidia-smi -l 1
```

### 4. Gi·∫£i ph√≥ng VRAM sau m·ªói run

```bash
# Tho√°t Python sau m·ªói l·∫ßn ch·∫°y
# VRAM s·∫Ω t·ª± ƒë·ªông gi·∫£i ph√≥ng
```

---

## üéÆ Ki·ªÉm tra GPU ƒëang ho·∫°t ƒë·ªông

Trong khi ch·∫°y inference, m·ªü terminal m·ªõi:

```bash
# Xem GPU usage
nvidia-smi

# Ho·∫∑c xem li√™n t·ª•c
nvidia-smi -l 1
```

B·∫°n s·∫Ω th·∫•y:
- **GPU-Util**: ~80-100% khi ƒëang x·ª≠ l√Ω
- **Memory-Usage**: TƒÉng l√™n ~2-3GB

---

## üìä V√≠ d·ª• Ch·∫°y Th·ª±c t·∫ø

### B∆∞·ªõc 1: Ch·∫°y inference v·ªõi GPU

```bash
python run.py infer \
    --dataset_dir ./train \
    --output predictions_gpu.json \
    --model clip \
    --detector yolov8l.pt \
    --threshold 0.45 \
    --conf 0.2 \
    --skip 1
```

### B∆∞·ªõc 2: ƒê√°nh gi√° k·∫øt qu·∫£

```bash
python run.py eval \
    --ground_truth ./train/annotations/annotations.json \
    --predictions predictions_gpu.json
```

### B∆∞·ªõc 3: Visualize

```bash
python visualize.py \
    --mode video \
    --video_path ./train/samples/Person1_0/drone_video.mp4 \
    --predictions predictions_gpu.json \
    --video_id Person1_0
```

---

## üî• L·ªánh Khuy√™n d√πng cho RTX 3050 4GB

```bash
# Ch·∫°y ngay b√¢y gi·ªù v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u
python run.py infer \
    --dataset_dir ./train \
    --output predictions_gpu_optimal.json \
    --model clip \
    --detector yolov8l.pt \
    --threshold 0.45 \
    --conf 0.2 \
    --skip 1

# Sau ƒë√≥ evaluate
python run.py eval \
    --ground_truth ./train/annotations/annotations.json \
    --predictions predictions_gpu_optimal.json \
    --output eval_gpu.json
```

**Th·ªùi gian ∆∞·ªõc t√≠nh:** ~5-10 ph√∫t cho to√†n b·ªô dataset (14 videos)

---

## ‚ö†Ô∏è X·ª≠ l√Ω L·ªói

### L·ªói: CUDA Out of Memory
```bash
RuntimeError: CUDA out of memory
```

**Gi·∫£i ph√°p:**
1. D√πng model nh·ªè h∆°n: `yolov8m.pt` thay v√¨ `yolov8x.pt`
2. D√πng ch·ªâ CLIP: `--model clip` thay v√¨ `--model both`
3. TƒÉng frame skip: `--skip 2` ho·∫∑c `--skip 3`
4. ƒê√≥ng c√°c ·ª©ng d·ª•ng kh√°c ƒëang d√πng GPU

### L·ªói: GPU kh√¥ng ƒë∆∞·ª£c d√πng
```bash
# Ki·ªÉm tra
python -c "import torch; print(torch.cuda.is_available())"
```

N·∫øu False, c√†i l·∫°i PyTorch v·ªõi CUDA:
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

---

## üìà K·ª≥ v·ªçng K·∫øt qu·∫£

V·ªõi GPU v√† c·∫•u h√¨nh t·ªëi ∆∞u:

| Metric | Gi√° tr·ªã |
|--------|---------|
| Mean STIoU | 0.4 - 0.6 |
| Processing Time | 5-10 ph√∫t |
| GPU Utilization | 80-100% |
| VRAM Usage | 2-3 GB |

---

**Ch√∫c b·∫°n th√†nh c√¥ng! üöÄ**
